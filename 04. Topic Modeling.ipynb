{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d7d48f-6b0f-412a-be9d-bd1a46a9a0f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/asamitakeuchi/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - pandas\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    conda-forge/label/gcc7::ca-certificat~ --> anaconda::ca-certificates-2022.4.26-hecd8cb5_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            pkgs/main::certifi-2023.7.22-py310hec~ --> anaconda::certifi-2022.6.15-py310hecd8cb5_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda pandas --yes\n",
    "!conda install -c anaconda seaborn --yes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as sp\n",
    "from scipy.stats import norm,skew,uniform\n",
    "from scipy import stats\n",
    "import datetime as dt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5142f5e-588c-48f5-9631-c0cafe1a9aa9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/asamitakeuchi/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - textblob\n",
      "\n",
      "\n",
      "The following packages will be UPDATED:\n",
      "\n",
      "  ca-certificates    anaconda::ca-certificates-2022.4.26-h~ --> conda-forge::ca-certificates-2023.7.22-h8857fd0_0 \n",
      "  certifi            anaconda/osx-64::certifi-2022.6.15-py~ --> conda-forge/noarch::certifi-2023.7.22-pyhd8ed1ab_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  openssl              pkgs/main::openssl-1.1.1v-hca72f7f_0 --> conda-forge::openssl-1.1.1v-h8a1eda9_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /Users/asamitakeuchi/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - wordcloud\n",
      "\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  ca-certificates    conda-forge::ca-certificates-2023.7.2~ --> conda-forge/label/gcc7::ca-certificates-2018.10.15-ha4d7672_0 \n",
      "  certifi            conda-forge/noarch::certifi-2023.7.22~ --> pkgs/main/osx-64::certifi-2023.7.22-py310hecd8cb5_0 \n",
      "  openssl            conda-forge::openssl-1.1.1v-h8a1eda9_0 --> pkgs/main::openssl-1.1.1v-hca72f7f_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c anaconda nltk --yes\n",
    "import nltk\n",
    "nltk.download ()\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "!conda install -c conda-forge textblob --yes\n",
    "from textblob import TextBlob\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "!conda install -c \"conda-forge/label/gcc7\" wordcloud --yes\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f41e54-0cc0-4963-b00b-eb7dcb7ad9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/asamitakeuchi/mercari/train2.csv')\n",
    "train_description = train['item_description']\n",
    "train_name = train['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddd523ef-23dd-4fc1-a719-7d8b0ede5aed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                    No description yet\n",
       "1     This keyboard is in great condition and works ...\n",
       "2     Adorable top with a hint of lace and a key hol...\n",
       "3     New with tags. Leather horses. Retail for [rm]...\n",
       "4             Complete with certificate of authenticity\n",
       "5     Banana republic bottoms, Candies skirt with ma...\n",
       "6     Size small but straps slightly shortened to fi...\n",
       "7     You get three pairs of Sophie cheer shorts siz...\n",
       "8      Girls Size small Plus green. Three shorts total.\n",
       "9     I realized his pants are on backwards after th...\n",
       "10         0.25 oz Full size is 1oz for [rm] in Sephora\n",
       "11    (5) new vs pink body mists (2.5 oz each) Fresh...\n",
       "12                                  Xl, great condition\n",
       "13                                   No description yet\n",
       "14    Authentic. Suede fringe boots. Great condition...\n",
       "15    Brand new. Deluxe travel size products. Contai...\n",
       "16    2 glitter eyeshadows; one in Brass and one in ...\n",
       "17    Brand new in box Size: Medium Color: Coral Ret...\n",
       "18    This AUTHENTIC pallete by Too Faced is brand n...\n",
       "19    Fancy, dressy or casual! Dress it up or down 1...\n",
       "Name: item_description, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_description[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b08cd6f-e415-46f4-b025-14b6c222e570",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.0    89717\n",
       "31.0     9587\n",
       "34.0     9534\n",
       "37.0     9505\n",
       "38.0     9374\n",
       "43.0     9364\n",
       "36.0     9361\n",
       "42.0     9328\n",
       "39.0     9308\n",
       "35.0     9280\n",
       "41.0     9250\n",
       "46.0     9236\n",
       "30.0     9234\n",
       "33.0     9208\n",
       "32.0     9202\n",
       "44.0     9188\n",
       "27.0     9188\n",
       "28.0     9128\n",
       "49.0     9123\n",
       "40.0     9113\n",
       "45.0     9098\n",
       "48.0     9084\n",
       "29.0     9082\n",
       "47.0     9028\n",
       "19.0     8996\n",
       "50.0     8967\n",
       "26.0     8961\n",
       "20.0     8846\n",
       "Name: item_description, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the ranking by the number of words of item_description\n",
    "desc_nums = train_description.apply(lambda x : len(x) if type(x) != float else x )\n",
    "desc_nums.value_counts()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "030c054f-4647-41f6-8344-7505e5c834f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the frequently available words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51cf5b72-17bc-4592-a903-193c49ab13eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_description))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066ef911-6237-42ee-ab31-7007ec798405",
   "metadata": {},
   "source": [
    "## NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d897a6ac-d3aa-40cb-8b3f-fe413178a2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/asamitakeuchi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/asamitakeuchi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/asamitakeuchi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                                            description yet\n",
      "1          keyboard great condition work like came box po...\n",
      "2          adorable top hint lace key hole back pale pink...\n",
      "3          new tag leather horse retail rm stand foot hig...\n",
      "4                          complete certificate authenticity\n",
      "                                 ...                        \n",
      "1482530    lace say size small fit medium perfectly never...\n",
      "1482531        little mermaid handmade dress never worn size\n",
      "1482532                         used twice still great shape\n",
      "1482533    one see red orange big red orange one world ma...\n",
      "1482534         new tag red sparkle firm price free shipping\n",
      "Name: cleaned_description, Length: 1482535, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "import string\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Assuming you have a DataFrame named 'train' with a column 'item_description'\n",
    "# Replace this with your actual DataFrame\n",
    "train_description = train['item_description']\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):  # Check if the value is a string\n",
    "        # Remove emojis and unicode text\n",
    "        text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "        \n",
    "        # Convert to lowercase\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', ' ', text)\n",
    "        \n",
    "        # Remove numbers\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "        \n",
    "        # Tokenize the text\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Remove stopwords\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [word for word in tokens if word not in stop_words]\n",
    "        \n",
    "        # Lemmatize the tokens\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        \n",
    "        # Join tokens back into a cleaned text\n",
    "        cleaned_text = ' '.join(tokens)\n",
    "        \n",
    "        return cleaned_text\n",
    "    else:\n",
    "        return \"\"  # Return an empty string for non-string values\n",
    "\n",
    "df = pd.DataFrame(train_description)\n",
    "df['cleaned_description'] = df['item_description'].apply(preprocess_text)\n",
    "\n",
    "# Display the cleaned descriptions\n",
    "print(df['cleaned_description'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9015b760-aed2-4bb3-87bc-9b838687d27b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('O')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['cleaned_description'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "925207cb-bd16-4635-b358-214a1e259e9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headline after vectorization: \n",
      "  (0, 193)\t0.19365162707570652\n",
      "  (0, 744)\t0.294861803679301\n",
      "  (0, 26)\t0.3235255570711431\n",
      "  (0, 428)\t0.149659225841063\n",
      "  (0, 472)\t0.1561894035179531\n",
      "  (0, 303)\t0.19483886516520854\n",
      "  (0, 607)\t0.1598610946681362\n",
      "  (0, 507)\t0.3276462527403772\n",
      "  (0, 304)\t0.20853654049901876\n",
      "  (0, 549)\t0.18380211916018072\n",
      "  (0, 133)\t0.25639635998403376\n",
      "  (0, 572)\t0.28111486633666966\n",
      "  (0, 192)\t0.20837628904837072\n",
      "  (0, 149)\t0.19728078259836337\n",
      "  (0, 737)\t0.1500251383527864\n",
      "  (0, 0)\t0.09918147265238658\n",
      "  (0, 427)\t0.1030670957171738\n",
      "  (0, 456)\t0.1078358581679075\n",
      "  (0, 596)\t0.21405690320850407\n",
      "  (0, 485)\t0.1259514963695362\n",
      "  (0, 286)\t0.2101243721475753\n",
      "  (0, 538)\t0.10516304995223984\n",
      "  (0, 113)\t0.11517782831884812\n",
      "  (0, 567)\t0.1086868549202091\n",
      "  (0, 171)\t0.19852588560721193\n",
      "  (0, 142)\t0.11342546931806118\n"
     ]
    }
   ],
   "source": [
    "#Vectorizing\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=10,\n",
    "                             max_features=180000,\n",
    "                             tokenizer=preprocess_text,\n",
    "                             ngram_range=(1, 2))\n",
    "description_matrix = tfidf_vectorizer.fit_transform(list(df['cleaned_description']))\n",
    "print('Headline after vectorization: \\n{}'.format(description_matrix[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3e62d5-f6a0-4072-941b-1eadfd97e4dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LDA\n",
    "lda_model = LatentDirichletAllocation(n_components=10,\n",
    "                                      learning_method='online',\n",
    "                                      max_iter=20,\n",
    "                                      random_state=42)\n",
    "X_topics = lda_model.fit_transform(description_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a443c0cd-a5d5-431b-83d0-bfb1b69b203c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "\n",
    "topic_word = lda_model.components_  # get the topic words\n",
    "vocab = tfidf_vectorizer .get_feature_names()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "    print('Topic {}: {}'.format(i, ' | '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0435393-a616-4b20-9c55-fd2605ce611d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "description_matrix.sort_values(by=['tfidf'], ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550934e4-7942-482e-9d3d-011a56d2c79e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf72390-a7c1-405c-a64a-ccde75698fe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a6be32-981f-443c-ba00-6aa1fda9b028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
